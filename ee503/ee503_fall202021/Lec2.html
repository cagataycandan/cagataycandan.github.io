<html xmlns:v="urn:schemas-microsoft-com:vml"
xmlns:o="urn:schemas-microsoft-com:office:office"
xmlns:w="urn:schemas-microsoft-com:office:word"
xmlns:st1="urn:schemas-microsoft-com:office:smarttags"
xmlns="http://www.w3.org/TR/REC-html40">

<head>
<meta http-equiv=Content-Type content="text/html; charset=windows-1252">
<meta name=ProgId content=Word.Document>
<meta name=Generator content="Microsoft Word 10">
<meta name=Originator content="Microsoft Word 10">
<link rel=File-List href="outline_files/filelist.xml">
<title>EE 503 Signal Analysis and Processing</title>
<o:SmartTagType namespaceuri="urn:schemas-microsoft-com:office:smarttags"
 name="place"/>
<o:SmartTagType namespaceuri="urn:schemas-microsoft-com:office:smarttags"
 name="City"/>
<o:SmartTagType namespaceuri="urn:schemas-microsoft-com:office:smarttags"
 name="State"/>
<!--[if gte mso 9]><xml>
 <o:DocumentProperties>
  <o:Author>METU-EEE</o:Author>
  <o:LastAuthor>EEE</o:LastAuthor>
  <o:Revision>2</o:Revision>
  <o:TotalTime>23</o:TotalTime>
  <o:Created>2007-09-21T10:56:00Z</o:Created>
  <o:LastSaved>2007-09-21T10:56:00Z</o:LastSaved>
  <o:Pages>1</o:Pages>
  <o:Words>750</o:Words>
  <o:Characters>4278</o:Characters>
  <o:Company>METU-EEE</o:Company>
  <o:Lines>35</o:Lines>
  <o:Paragraphs>10</o:Paragraphs>
  <o:CharactersWithSpaces>5018</o:CharactersWithSpaces>
  <o:Version>10.2625</o:Version>
 </o:DocumentProperties>
</xml><![endif]--><!--[if gte mso 9]><xml>
 <w:WordDocument>
  <w:SpellingState>Clean</w:SpellingState>
  <w:GrammarState>Clean</w:GrammarState>
  <w:Compatibility>
   <w:BreakWrappedTables/>
   <w:SnapToGridInCell/>
   <w:WrapTextWithPunct/>
   <w:UseAsianBreakRules/>
  </w:Compatibility>
  <w:BrowserLevel>MicrosoftInternetExplorer4</w:BrowserLevel>
 </w:WordDocument>
</xml><![endif]--><!--[if !mso]><object
 classid="clsid:38481807-CA0E-42D2-BF39-B33AF135CC4D" id=ieooui></object>
<style>
st1\:*{behavior:url(#ieooui) }
</style>
<![endif]-->
<style>
<!--
 /* Font Definitions */
 @font-face
	{font-family:Wingdings;
	panose-1:5 0 0 0 0 0 0 0 0 0;
	mso-font-charset:2;
	mso-generic-font-family:auto;
	mso-font-pitch:variable;
	mso-font-signature:0 268435456 0 0 -2147483648 0;}
 /* Style Definitions */
 p.MsoNormal, li.MsoNormal, div.MsoNormal
	{mso-style-parent:"";
	margin:0cm;
	margin-bottom:.0001pt;
	mso-pagination:widow-orphan;
	font-size:12.0pt;
	font-family:"Times New Roman";
	mso-fareast-font-family:"Times New Roman";}
span.SpellE
	{mso-style-name:"";
	mso-spl-e:yes;}
span.GramE
	{mso-style-name:"";
	mso-gram-e:yes;}
@page Section1
	{size:612.0pt 792.0pt;
	margin:72.0pt 90.0pt 72.0pt 90.0pt;
	mso-header-margin:35.4pt;
	mso-footer-margin:35.4pt;
	mso-paper-source:0;}
div.Section1
	{page:Section1;}
 /* List Definitions */
 @list l0
	{mso-list-id:24134985;
	mso-list-type:hybrid;
	mso-list-template-ids:1418224520 67698703 67698713 67698715 67698703 67698713 67698715 67698703 67698713 67698715;}
@list l0:level1
	{mso-level-tab-stop:36.0pt;
	mso-level-number-position:left;
	text-indent:-18.0pt;}
@list l0:level2
	{mso-level-number-format:alpha-lower;
	mso-level-tab-stop:72.0pt;
	mso-level-number-position:left;
	text-indent:-18.0pt;}
@list l0:level3
	{mso-level-number-format:roman-lower;
	mso-level-tab-stop:108.0pt;
	mso-level-number-position:right;
	text-indent:-9.0pt;}
@list l0:level4
	{mso-level-tab-stop:144.0pt;
	mso-level-number-position:left;
	text-indent:-18.0pt;}
@list l0:level5
	{mso-level-number-format:alpha-lower;
	mso-level-tab-stop:180.0pt;
	mso-level-number-position:left;
	text-indent:-18.0pt;}
ol
	{margin-bottom:0cm;}
ul
	{margin-bottom:0cm;}
div.MsoNormal1 {mso-style-parent:"";
	margin:0cm;
	margin-bottom:.0001pt;
	mso-pagination:widow-orphan;
	font-size:12.0pt;
	font-family:"Times New Roman";
	mso-fareast-font-family:"Times New Roman";}
li.MsoNormal1 {mso-style-parent:"";
	margin:0cm;
	margin-bottom:.0001pt;
	mso-pagination:widow-orphan;
	font-size:12.0pt;
	font-family:"Times New Roman";
	mso-fareast-font-family:"Times New Roman";}
p.MsoNormal1 {mso-style-parent:"";
	margin:0cm;
	margin-bottom:.0001pt;
	mso-pagination:widow-orphan;
	font-size:12.0pt;
	font-family:"Times New Roman";
	mso-fareast-font-family:"Times New Roman";}
.style1 {font-size: 14px}
.style2 {color: #FF0000}
-->
</style>
<!--[if gte mso 10]>
<style>
 /* Style Definitions */
 table.MsoNormalTable
	{mso-style-name:"Table Normal";
	mso-tstyle-rowband-size:0;
	mso-tstyle-colband-size:0;
	mso-style-noshow:yes;
	mso-style-parent:"";
	mso-padding-alt:0cm 5.4pt 0cm 5.4pt;
	mso-para-margin:0cm;
	mso-para-margin-bottom:.0001pt;
	mso-pagination:widow-orphan;
	font-size:10.0pt;
	font-family:"Times New Roman";}
</style>
<![endif]--><!--[if gte mso 9]><xml>
 <o:shapedefaults v:ext="edit" spidmax="2050"/>
</xml><![endif]--><!--[if gte mso 9]><xml>
 <o:shapelayout v:ext="edit">
  <o:idmap v:ext="edit" data="1"/>
 </o:shapelayout></xml><![endif]-->
</head>

<body lang=EN-US style='tab-interval:36.0pt'>

<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-37099228-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>

<!--
https://wordhtml.com/
https://www.w3schools.com/html/tryit.asp?filename=tryhtml_basic

https://wordpress.stackexchange.com/questions/360307/possibility-to-control-embedded-video-timeline-with-buttons-and-links-external-t
-->

<div class=Section1>
  <p>&nbsp;</p>
  <div align="center">
	<table width="75%" border="0"><tr>
  <td align="left"><a href="Lec1.html"><<< Previous 10 Lectures <<< </a></td>
  <td align="right"><a href="Lec3.html">>>> Next 10 Lectures >>> </a></td>
  </tr>
  </table>
  
  <table width="75%" border="1">
    <tr> 
      <td colspan="3"><div align="center"> 
          <p><strong><em><font color="#CC0000">EE 503 Lectures (Fall 2020/21)</font></em></strong></p>
        </div></td>
    </tr>
    <tr> 
      <td width="11%">Lec. #11 </td>
      <td width="67%"> <p> <a target="Lec11YT" href="https://www.youtube.com/embed/-EyBewUPkZk?start=1&autoplay=1">00:00</a> 
          - "similarity" of two r.v.'s <br />
          <a target="Lec11YT" href="https://www.youtube.com/embed/-EyBewUPkZk?start=45&autoplay=1">00:45</a> 
          - Inner product as an indicator of "similarity" for N-dim vectors (reminder) 
          <br />
          <a target="Lec11YT" href="https://www.youtube.com/embed/-EyBewUPkZk?start=254&autoplay=1">04:14</a> 
          - Correlation coefficient (definition for zero mean r.v.'s) <br />
          <a target="Lec11YT" href="https://www.youtube.com/embed/-EyBewUPkZk?start=508&autoplay=1">08:28</a> 
          - Properties of correlation coefficient <br />
          <a target="Lec11YT" href="https://www.youtube.com/embed/-EyBewUPkZk?start=571&autoplay=1">09:31</a> 
          - |r_{xy}| \leq 1 (proof) <br />
          <a target="Lec11YT" href="https://www.youtube.com/embed/-EyBewUPkZk?start=815&autoplay=1">13:35</a> 
          - |r_{xy}| = 1 implies Y = aX <br />
          <a target="Lec11YT" href="https://www.youtube.com/embed/-EyBewUPkZk?start=1001&autoplay=1">16:41</a> 
          - Y = aX imples |r_{xy}| = 1 <br />
          <a target="Lec11YT" href="https://www.youtube.com/embed/-EyBewUPkZk?start=1203&autoplay=1">20:03</a> 
          - r_{xy} as angle between two r.v.'s <br />
          <a target="Lec11YT" href="https://www.youtube.com/embed/-EyBewUPkZk?start=1321&autoplay=1">22:01</a> 
          - Orthgonal r.v.'s (definition) <br />
          <a target="Lec11YT" href="https://www.youtube.com/embed/-EyBewUPkZk?start=1385&autoplay=1">23:05</a> 
          - Example: r_{xy} calculation for Y = aX + N <br />
          <a target="Lec11YT" href="https://www.youtube.com/embed/-EyBewUPkZk?start=1792&autoplay=1">29:52</a> 
          - SNR definition (example continues) <br />
          <a target="Lec11YT" href="https://www.youtube.com/embed/-EyBewUPkZk?start=1975&autoplay=1">32:55</a> 
          - Interpretation of results (example continues) <br />
          <a target="Lec11YT" href="https://www.youtube.com/embed/-EyBewUPkZk?start=2260&autoplay=1">37:40</a> 
          - Correlation (definition) <br />
          <a target="Lec11YT" href="https://www.youtube.com/embed/-EyBewUPkZk?start=2302&autoplay=1">38:22</a> 
          - Correlation coefficient (definition) <br />
          <a target="Lec11YT" href="https://www.youtube.com/embed/-EyBewUPkZk?start=2545&autoplay=1">42:25</a> 
          - Cov(X,Y) (definition) <br />
          <a target="Lec11YT" href="https://www.youtube.com/embed/-EyBewUPkZk?start=2720&autoplay=1">45:20</a> 
          - Uncorrelated r.v.'s <br />
          <a target="Lec11YT" href="https://www.youtube.com/embed/-EyBewUPkZk?start=2855&autoplay=1">47:35</a> 
          - Example: Cov(X,Y) for r.v.'s X and Y which are indicator functions 
          events A and B <br />
          <a target="Lec11YT" href="https://www.youtube.com/embed/-EyBewUPkZk?start=3303&autoplay=1">55:03</a> 
          - Interpretation of results (example continues) <br />
          <a target="Lec11YT" href="https://www.youtube.com/embed/-EyBewUPkZk?start=3471&autoplay=1">57:51</a> 
          - Properties of Cov(X,Y) <br />
          <a target="Lec11YT" href="https://www.youtube.com/embed/-EyBewUPkZk?start=3658&autoplay=1">1:00:58</a> 
          - Example: Var ( \sum_{i=1}^N x_i ) <br />
          <a target="Lec11YT" href="https://www.youtube.com/embed/-EyBewUPkZk?start=3980&autoplay=1">1:06:20</a> 
          - Independence implies uncorrelatedness (proof)</p></td>
      <td width="22%"> <iframe name="Lec11YT" width="560" height="315" src="https://www.youtube.com/embed/-EyBewUPkZk" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
    </tr>
    <tr> 
      <td>Lec. #12a</td>
      <td><p> A Matlab illustration on the correlation coefficient <br>
          <br>
          <a href="lecture_notes/On_Correlation%20_Coefficient.pdf">.pdf document</a> 
        </p></td>
      <td> <iframe name="Lec12a" width="560" height="315" src="https://www.youtube.com/embed/rP-9EOVHd6s" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
    <tr> 
      <td>Lec. #12b</td>
      <td><p> <a target="Lec12b" href="https://www.youtube.com/embed/58tkv0Ncgo8?start=1&autoplay=1">00:00</a> 
          - Random vectors <br />
          <a target="Lec12b" href="https://www.youtube.com/embed/58tkv0Ncgo8?start=184&autoplay=1">03:04</a> 
          - Correlation matrix (definition) <br />
          <a target="Lec12b" href="https://www.youtube.com/embed/58tkv0Ncgo8?start=211&autoplay=1">03:31</a> 
          - Covariance matrix (definition) <br />
          <a target="Lec12b" href="https://www.youtube.com/embed/58tkv0Ncgo8?start=670&autoplay=1">11:10</a> 
          - Properties of covariance matrix <br />
          <a target="Lec12b" href="https://www.youtube.com/embed/58tkv0Ncgo8?start=734&autoplay=1">12:14</a> 
          - Hermitian symmetry (properties continue) <br />
          <a target="Lec12b" href="https://www.youtube.com/embed/58tkv0Ncgo8?start=918&autoplay=1">15:18</a> 
          - Positive semi-definiteness (properties continue) <br />
          <a target="Lec12b" href="https://www.youtube.com/embed/58tkv0Ncgo8?start=1247&autoplay=1">20:47</a> 
          - Gaussian Distribution <br />
          <a target="Lec12b" href="https://www.youtube.com/embed/58tkv0Ncgo8?start=1276&autoplay=1">21:16</a> 
          - 1D Gaussian r.v. <br />
          <a target="Lec12b" href="https://www.youtube.com/embed/58tkv0Ncgo8?start=1690&autoplay=1">28:10</a> 
          - N-dimensional Gaussian vectors <br />
          <a target="Lec12b" href="https://www.youtube.com/embed/58tkv0Ncgo8?start=1988&autoplay=1">33:08</a> 
          - 2-dimensional Gaussian vectors <br />
          <a target="Lec12b" href="https://www.youtube.com/embed/58tkv0Ncgo8?start=2415&autoplay=1">40:15</a> 
          - Level curves (2D Gaussian vectors) <br />
          <a target="Lec12b" href="https://www.youtube.com/embed/58tkv0Ncgo8?start=2715&autoplay=1">45:15</a> 
          - Level curves (2D Gaussian vector, Cx : diagonal) <br />
          <a target="Lec12b" href="https://www.youtube.com/embed/58tkv0Ncgo8?start=2950&autoplay=1">49:10</a> 
          - Level curves (2D Gaussian vector, Cx \propto I ) <br />
          <a target="Lec12b" href="https://www.youtube.com/embed/58tkv0Ncgo8?start=3030&autoplay=1">50:30</a> 
          - Facts on Gaussian vectors <br />
          <a target="Lec12b" href="https://www.youtube.com/embed/58tkv0Ncgo8?start=3045&autoplay=1">50:45</a> 
          - Marginalization (facts continue) <br />
          <a target="Lec12b" href="https://www.youtube.com/embed/58tkv0Ncgo8?start=3168&autoplay=1">52:48</a> 
          - Example on marginalization of Gaussian vectors <br />
          <a target="Lec12b" href="https://www.youtube.com/embed/58tkv0Ncgo8?start=3330&autoplay=1">55:30</a> 
          - Linear processing of Gaussian vectors (facts continue) <br />
          <a target="Lec12b" href="https://www.youtube.com/embed/58tkv0Ncgo8?start=3375&autoplay=1">56:15</a> 
          - Example: Ry matrix in terms of Rx for y = Mx <br />
          <a target="Lec12b" href="https://www.youtube.com/embed/58tkv0Ncgo8?start=3733&autoplay=1">1:02:13</a> 
          - Example: Var ( \sum_{i=1}^N x_i ) (redo earlier example with vector 
          operations) </p>
        <br> <strong>Corrections:</strong><br>
        36:20 - 2D Gaussian case: pdf (2nd line on the left side) should have 
        Cx^{-1} not Cx (Cagatay C.) </td>
      <td> <iframe name="Lec12b" width="560" height="315" src="https://www.youtube.com/embed/58tkv0Ncgo8" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
    <tr> 
      <td>Lec. #13</td>
      <td><p> <a target="Lec13YT" href="https://www.youtube.com/embed/PVOo4vH0mU0?start=1&autoplay=1">0:00</a> 
          - Linear processing of random vectors (summary) <br />
          <a target="Lec13YT" href="https://www.youtube.com/embed/PVOo4vH0mU0?start=220&autoplay=1">3:40</a> 
          - Decorrelation of random vectors <br />
          <a target="Lec13YT" href="https://www.youtube.com/embed/PVOo4vH0mU0?start=345&autoplay=1">5:45</a> 
          - Case 1: Diagonalization by eigendecomposition <br />
          <a target="Lec13YT" href="https://www.youtube.com/embed/PVOo4vH0mU0?start=1155&autoplay=1">19:15</a> 
          - Case 2: Diagonalization by unitary transformation and scaling <br />
          <a target="Lec13YT" href="https://www.youtube.com/embed/PVOo4vH0mU0?start=1415&autoplay=1">23:35</a> 
          - Case 3: Diagonalization by LU decomposition <br />
          <a target="Lec13YT" href="https://www.youtube.com/embed/PVOo4vH0mU0?start=1769&autoplay=1">29:29</a> 
          - Unit lower triangular matrix (definition) <br />
          <a target="Lec13YT" href="https://www.youtube.com/embed/PVOo4vH0mU0?start=2187&autoplay=1">36:27</a> 
          - Causal decorrelation operation by LU decomposition <br />
      </td>
      <td> <iframe name="Lec13YT" width="560" height="315" src="https://www.youtube.com/embed/MBWHnaitu7g" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
    </tr>
    <tr> 
      <td>Lec. #14 </td>
      <td><p> <a target="Lec14YT" href="https://www.youtube.com/embed/n94pMkNahmk?start=1&autoplay=1">0:00</a> 
          - Diagonalization of Covariance Matrices (review) <br />
          <a target="Lec14YT" href="https://www.youtube.com/embed/n94pMkNahmk?start=230&autoplay=1">3:50</a> 
          - Diagonalization by eigendecomposition (review) <br />
          <a target="Lec14YT" href="https://www.youtube.com/embed/n94pMkNahmk?start=361&autoplay=1">06:01</a> 
          - Diagonalization by unitary Transformation and followed by scaling 
          (review) <br />
          <a target="Lec14YT" href="https://www.youtube.com/embed/n94pMkNahmk?start=515&autoplay=1">08:35</a> 
          - Diagonalization by LU decomposition (review) <br />
          <a target="Lec14YT" href="https://www.youtube.com/embed/n94pMkNahmk?start=700&autoplay=1">11:40</a> 
          - Joint Diagonalization of two covariance matrices <br />
          <a target="Lec14YT" href="https://www.youtube.com/embed/n94pMkNahmk?start=913&autoplay=1">15:13</a> 
          - Joint Diagonalization of two covariance matrices (Step 1) <br />
          <a target="Lec14YT" href="https://www.youtube.com/embed/n94pMkNahmk?start=1094&autoplay=1">18:14</a> 
          - Joint Diagonalization of two covariance matrices (Step 2) <br />
          <a target="Lec14YT" href="https://www.youtube.com/embed/n94pMkNahmk?start=1595&autoplay=1">26:35</a> 
          - Joint Diagonalization of two covariance matrices (with a single step) 
          <br />
          <a target="Lec14YT" href="https://www.youtube.com/embed/n94pMkNahmk?start=2450&autoplay=1">40:50</a> 
          - Random processes <br />
        </p></td>
      <td> <iframe name="Lec14YT" width="560" height="315" src="https://www.youtube.com/embed/n94pMkNahmk" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
    <tr> 
      <td>Lec. #15</td>
      <td><p> <a target="Lec15YT" href="https://www.youtube.com/embed/Mv6-TJp_VwY?start=1&autoplay=1">0:00</a> 
          - Descriptions for Random Processes <br />
          <a target="Lec15YT" href="https://www.youtube.com/embed/Mv6-TJp_VwY?start=35&autoplay=1">0:35</a> 
          - Joint pdf description <br />
          <a target="Lec15YT" href="https://www.youtube.com/embed/Mv6-TJp_VwY?start=402&autoplay=1">6:42</a> 
          - Example: joint pdf description of x(t) = A cos(2\pi f t + \theta), 
          \theta: r.v. ~ Unif. [0,2\pi) <br />
          <a target="Lec15YT" href="https://www.youtube.com/embed/Mv6-TJp_VwY?start=730&autoplay=1">12:10</a> 
          - 1st order pdf description (example) <br />
          <a target="Lec15YT" href="https://www.youtube.com/embed/Mv6-TJp_VwY?start=802&autoplay=1">13:22</a> 
          - One function of one r.v. discussion <br />
          <a target="Lec15YT" href="https://www.youtube.com/embed/Mv6-TJp_VwY?start=1607&autoplay=1">26:47</a> 
          - 1st order pdf description (example, continued)<br />
          <a target="Lec15YT" href="https://www.youtube.com/embed/Mv6-TJp_VwY?start=2232&autoplay=1">37:12</a> 
          - 1st order pdf description (comments) <br />
          <a target="Lec15YT" href="https://www.youtube.com/embed/Mv6-TJp_VwY?start=2511&autoplay=1">41:51</a> 
          - 2nd order pdf description (example) <br />
          <a target="Lec15YT" href="https://www.youtube.com/embed/Mv6-TJp_VwY?start=3531&autoplay=1">58:51</a> 
          - 3rd order pdf description (example) <br />
        </p></td>
      <td> <iframe name="Lec15YT" width="560" height="315" src="https://www.youtube.com/embed/Mv6-TJp_VwY" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
    <tr> 
      <td>Lec. #16</td>
      <td><p> <a target="Lec16YT" href="https://www.youtube.com/embed/V3njAinVvMc?start=1&autoplay=1">0:00</a> 
          - Random Process Descriptions (cont'd) <br />
          <a target="Lec16YT" href="https://www.youtube.com/embed/V3njAinVvMc?start=58&autoplay=1">0:58</a> 
          - Gaussian Processes <br />
          <a target="Lec16YT" href="https://www.youtube.com/embed/V3njAinVvMc?start=140&autoplay=1">2:20</a> 
          - Joint pdf of samples (Gaussian processes) <br />
          <a target="Lec16YT" href="https://www.youtube.com/embed/V3njAinVvMc?start=360&autoplay=1">6:00</a> 
          - Example: Marginal pdf's are Gaussian, while joint pdf is not <br />
          <a target="Lec16YT" href="https://www.youtube.com/embed/V3njAinVvMc?start=890&autoplay=1">14:50</a> 
          - Example: x(t) = a t+ b (a,b: r.v.'s) <br />
          <a target="Lec16YT" href="https://www.youtube.com/embed/V3njAinVvMc?start=1153&autoplay=1">19:13</a> 
          - 1st order pdf description (example) <br />
          <a target="Lec16YT" href="https://www.youtube.com/embed/V3njAinVvMc?start=1625&autoplay=1">27:05</a> 
          - 2nd order pdf description (example) <br />
          <a target="Lec16YT" href="https://www.youtube.com/embed/V3njAinVvMc?start=2260&autoplay=1">37:40</a> 
          - 3rd order pdf description (example) <br />
          <a target="Lec16YT" href="https://www.youtube.com/embed/V3njAinVvMc?start=2547&autoplay=1">42:27</a> 
          - Discussion of degenerate cov. mat. (example) <br />
        </p></td>
      <td><iframe name="Lec16YT" width="560" height="315" src="https://www.youtube.com/embed/V3njAinVvMc" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
    <tr> 
      <td>Lec. #17</td>
      <td><p> <a target="Lec17YT" href="https://www.youtube.com/embed/n0QMJPUNB5k?start=15&autoplay=1">0:15</a> 
          - Moments Description <br />
          <a target="Lec17YT" href="https://www.youtube.com/embed/n0QMJPUNB5k?start=155&autoplay=1">2:35</a> 
          - 1st Order Moment Description: mean function <br />
          <a target="Lec17YT" href="https://www.youtube.com/embed/n0QMJPUNB5k?start=199&autoplay=1">3:19</a> 
          - 2nd Order Moment Description: Autocorrelation/Covariance functions 
          <br />
          <a target="Lec17YT" href="https://www.youtube.com/embed/n0QMJPUNB5k?start=308&autoplay=1">5:08</a> 
          - Comments: on the Moments Descriptions <br />
          <a target="Lec17YT" href="https://www.youtube.com/embed/n0QMJPUNB5k?start=558&autoplay=1">9:18</a> 
          - Example: x(t) a r.p. with mu_x(t)=3 and R_x(t1,t2)=9+4*exp(-0.2|t1-t2|), 
          z=x(5), w=x(8), Find E{z}, E{w}, E{z^2}, E{w^2} and E{zw} <br />
          <a target="Lec17YT" href="https://www.youtube.com/embed/n0QMJPUNB5k?start=890&autoplay=1">14:50</a> 
          - Example: z = x(t1) + x(t2), Find E{z^2} <br />
          <a target="Lec17YT" href="https://www.youtube.com/embed/n0QMJPUNB5k?start=1032&autoplay=1">17:12</a> 
          - Example: s = integral_{from a to b}(x(t)dt), a) Find E{s}, b) Find 
          E{s^2} <br />
          <a target="Lec17YT" href="https://www.youtube.com/embed/n0QMJPUNB5k?start=1293&autoplay=1">21:33</a> 
          - Example: x(t)=Acos(wt+Q), A,Q are independent r.v.'s, Q~uniform[0,2*pi), 
          Find mu_x(t) and R_x(t1,t2) <br />
          <a target="Lec17YT" href="https://www.youtube.com/embed/n0QMJPUNB5k?start=2054&autoplay=1">34:14</a> 
          - Notes: about complex valued processes <br />
          <a target="Lec17YT" href="https://www.youtube.com/embed/n0QMJPUNB5k?start=2233&autoplay=1">37:13</a> 
          - Example: x(t)=A*exp(wt+Q), A,Q are independent r.v.'s, Q~uniform[0,2*pi), 
          Find mu_x(t) and R_x(t1,t2) <br />
        </p></td>
      <td><iframe name="Lec17YT" width="560" height="315" src="https://www.youtube.com/embed/n0QMJPUNB5k" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
    <tr> 
      <td>Lec. #18</td>
      <td><p> <a target="Lec18YT" href="https://www.youtube.com/embed/fpTgUgNBGso?start=1&autoplay=1">0:00</a> 
          - White noise process <br />
          <a target="Lec18YT" href="https://www.youtube.com/embed/fpTgUgNBGso?start=490&autoplay=1">8:10</a> 
          - Example: x_1[k] = w and x_2[k] = w_k <br />
          <a target="Lec18YT" href="https://www.youtube.com/embed/fpTgUgNBGso?start=857&autoplay=1">14:17</a> 
          - Joint pdf description (example) <br />
          <a target="Lec18YT" href="https://www.youtube.com/embed/fpTgUgNBGso?start=1275&autoplay=1">21:15</a> 
          - Moment description (example) <br />
          <a target="Lec18YT" href="https://www.youtube.com/embed/fpTgUgNBGso?start=1805&autoplay=1">30:05</a> 
          - Example: w_1[n] \in {1,-1} iid; w_2[n] ~ N(0,1), iid <br />
          <a target="Lec18YT" href="https://www.youtube.com/embed/fpTgUgNBGso?start=2030&autoplay=1">33:50</a> 
          - Joint pdf description (example) <br />
          <a target="Lec18YT" href="https://www.youtube.com/embed/fpTgUgNBGso?start=2310&autoplay=1">38:30</a> 
          - Moment description (example) <br />
          space</p></td>
      <td> <iframe name="Lec18YT" width="560" height="315" src="https://www.youtube.com/embed/fpTgUgNBGso" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
    <tr> 
      <td>Lec. #19</td>
      <td><p><a target="Lec19" href="https://www.youtube.com/embed/QMjMaqVehec?start=1&autoplay=1">0:00</a> 
          - Linear systems with stochastic inputs <br />
          <a target="Lec19" href="https://www.youtube.com/embed/QMjMaqVehec?start=33&autoplay=1">0:33</a> 
          - Linear systems - continuous time (review) <br />
          <a target="Lec19" href="https://www.youtube.com/embed/QMjMaqVehec?start=136&autoplay=1">2:16</a> 
          - LTI systems - continuous time (review) <br />
          <a target="Lec19" href="https://www.youtube.com/embed/QMjMaqVehec?start=306&autoplay=1">5:06</a> 
          - Linear systems - discrete time (review) <br />
          <a target="Lec19" href="https://www.youtube.com/embed/QMjMaqVehec?start=460&autoplay=1">7:40</a> 
          - LTI systems - discrete time (review) <br />
          <a target="Lec19" href="https://www.youtube.com/embed/QMjMaqVehec?start=538&autoplay=1">8:58</a> 
          - LTI systems - discrete time, convolution matrix (review) <br />
          <a target="Lec19" href="https://www.youtube.com/embed/QMjMaqVehec?start=671&autoplay=1">11:11</a> 
          - Moment descriptions for linear systems with stochastic inputs <br />
          <a target="Lec19" href="https://www.youtube.com/embed/QMjMaqVehec?start=690&autoplay=1">11:30</a> 
          - Linear processing output mean function calculation <br />
          <a target="Lec19" href="https://www.youtube.com/embed/QMjMaqVehec?start=842&autoplay=1">14:02</a> 
          - Basic Assumption: Commutation of Expectation operation and Linear 
          operations <br />
          <a target="Lec19" href="https://www.youtube.com/embed/QMjMaqVehec?start=989&autoplay=1">16:29</a> 
          - Linear processing output auto-correlation function calculation <br />
          <a target="Lec19" href="https://www.youtube.com/embed/QMjMaqVehec?start=1021&autoplay=1">17:01</a> 
          - Linear processing output auto-correlation function calculation (step 
          1: cross correlation) <br />
          <a target="Lec19" href="https://www.youtube.com/embed/QMjMaqVehec?start=1510&autoplay=1">25:10</a> 
          - Linear processing output auto-correlation function calculation (step 
          2: auto-correlation) <br />
          <a target="Lec19" href="https://www.youtube.com/embed/QMjMaqVehec?start=1924&autoplay=1">32:04</a> 
          - Example: y(t) = L{x(t)} = d/dt x(t), Find output mean and autocorrelation 
          functions <br />
          <a target="Lec19" href="https://www.youtube.com/embed/QMjMaqVehec?start=2460&autoplay=1">41:00</a> 
          - Connections with earlier finite dimensional results <br />
          <a target="Lec19" href="https://www.youtube.com/embed/QMjMaqVehec?start=2832&autoplay=1">47:12</a> 
          - Brief discussion on LTI and WSS input case <br />
          <a target="Lec19" href="https://www.youtube.com/embed/QMjMaqVehec?start=3195&autoplay=1">53:15</a> 
          - Stationary Random Processes <br />
          <a target="Lec19" href="https://www.youtube.com/embed/QMjMaqVehec?start=3483&autoplay=1">58:03</a> 
          - Stationarity in joint pdf description <br />
          <a target="Lec19" href="https://www.youtube.com/embed/QMjMaqVehec?start=3727&autoplay=1">1:02:07</a> 
          - Stationarity in moment description </p></td>
      <td><iframe name="Lec19" width="560" height="315" src="https://www.youtube.com/embed/QMjMaqVehec" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
    <tr> 
      <td>Lec. #20</td>
      <td><p> <a target="Lec20YT" href="https://www.youtube.com/embed/LkZ8nNK5c94?start=1&autoplay=1">0:00</a> 
          - Stationarity in pdf/moment descriptions (review) <br />
          <a target="Lec20YT" href="https://www.youtube.com/embed/LkZ8nNK5c94?start=640&autoplay=1">10:40</a> 
          - SSS implies WSS <br />
          <a target="Lec20YT" href="https://www.youtube.com/embed/LkZ8nNK5c94?start=708&autoplay=1">11:48</a> 
          - Gaussian process and WSS is equivalent to SSS <br />
          <a target="Lec20YT" href="https://www.youtube.com/embed/LkZ8nNK5c94?start=821&autoplay=1">13:41</a> 
          - Example: x(t) = a cos(wt) + b sin(wt), find conditions on a,b r.v.'s 
          for x(t) be WSS <br />
          <a target="Lec20YT" href="https://www.youtube.com/embed/LkZ8nNK5c94?start=870&autoplay=1">14:30</a> 
          - Stationarity in the mean (example, cont'd) <br />
          <a target="Lec20YT" href="https://www.youtube.com/embed/LkZ8nNK5c94?start=1191&autoplay=1">19:51</a> 
          - Stationarity in the autocorrelation (example, cont'd) <br />
          <a target="Lec20YT" href="https://www.youtube.com/embed/LkZ8nNK5c94?start=1958&autoplay=1">32:38</a> 
          - Example: x[n] r.p with inpendent sample with x[2n] ~ Unif(-\sqrt(3),\sqrt(3) 
          and x[2n+1] ~ N(0,1). Is x[n] WSS/SSS? <br />
          <a target="Lec20YT" href="https://www.youtube.com/embed/LkZ8nNK5c94?start=2533&autoplay=1">42:13</a> 
          - Jointly WSS random processes <br />
          <a target="Lec20YT" href="https://www.youtube.com/embed/LkZ8nNK5c94?start=2710&autoplay=1">45:10</a> 
          - LTI processing of WSS processes <br />
          <a target="Lec20YT" href="https://www.youtube.com/embed/LkZ8nNK5c94?start=2825&autoplay=1">47:05</a> 
          - Mean function calculation (LTI processing of WSS processes) <br />
          space</p>
        <p><strong>Corrections:</strong> <br>
          39:38- left board, very bottom line should be \delta[k] instead of \delta[n-k] 
          (Thanks to Gulin T.)</p></td>
      <td><iframe name="Lec20YT" width="560" height="315" src="https://www.youtube.com/embed/LkZ8nNK5c94" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
  </table>
  <table width="75%" border="0"><tr>
  <td align="left"><a href="Lec1.html"><<< Previous 10 Lectures <<< </a></td>
  <td align="right"><a href="Lec3.html">>>> Next 10 Lectures >>> </a></td>
  </tr></table>
</div>



<div class=Section1 style2>

<p class=MsoNormal align=center style='text-align:center'>&nbsp;</p>
<p align=center class=MsoNormal  style='text-align:center'>
  <o:p>&nbsp;</o:p>
</p>
  <p class=MsoNormal> <o:p>&nbsp;</o:p>
  <p class=MsoNormal>&nbsp;
  <p class=MsoNormal>&nbsp;
  <p class=MsoNormal>&nbsp;
  <div align="center"><strong>EE 503 Statistical Signal Processing and Modeling
    </strong><br>
    <strong>(Fall 2019&ndash; 2020)</strong></div>
  </p>

<table align=center width=75% border="0">
    <tr>
      <td><p class="style1"><strong>Short Description: </strong></p>
        <p class="style1"> This course is the first course on statistical signal
          processing in the graduate curriculum of Department of Electrical and
          Electronics Engineering, Middle East Technical University (METU). Topics
          covered in this course are random vectors, random processes, stationary
          random processes, wide sense stationary processes and their processing
          with LTI systems with applications in optimal filtering, smoothing and
          prediction. A major goal is to introduce the concept of mean square
          error (MSE) optimal processing of random signals by LTI systems. <br>
          <br>
          For the processing of the random signals, it is assumed that some statistical
          information about the signal of interest and distortion is known. By
          utilizing this information, MSE optimal LTI filters (Wiener filters)
          are designed. This forms the processing part of the course. The estimation
          of the statistical information to construct Wiener filters forms the
          modeling part of the course. In the modeling part, we examine AR, MA,
          ARMA models for random signals and give a brief discussion of Pade,
          Prony methods for the deterministic modeling. Among other topics of
          importance are decorrelating transforms (whitening), spectral factorization,
          Karhunen-Loeve transform <br>
          <br>
          This course is a natural pre-requisite (not a formal one) to EE5506
          Advanced Statistical Signal Processing. The estimation theory topics
          in EE 503 is mostly limited to the moment description of random processes
          which forms a special, but the most important, case of EE 5506. </p>
        <p class="style1"><strong>Outline of Topics: </strong></p>
        <ol start="1" type="1" class="style1">
          <li>Review</li>
          <ol start="1" type="a">
            <li> Basics of Mathematical Deduction
              <ol>
                <li> Necessary, Sufficient Conditions</li>
                <li> Proofs via contradiction, contraposition</li>
              </ol>
            </li>
            <li>Basics of Linear Algebra
              <ol>
                <li>Linear independence of vectors (points in linear space)</li>
                <li>Range and Null space of the combination process</li>
                <li> Projection to Range/Null Space (orthogonality principle)
                </li>
                <li>Positive Definite Matrices</li>
              </ol>
            </li>
            <li>Basics of Probability
              <ol>
                <li> Probability as a mapping, axioms, conditional probability</li>
                <li>Expectation, law of large numbers</li>
                <li>Moments, moment generating function</li>
              </ol>
            </li>
          </ol>
          <br>
          <li>Random Processes
            <ol>
              <li>Random variables, random vectors (or a sequence of random variables),
                moment descriptions (mean, variance, correlation), decorrelating
                transforms</li>
              <li>Random processes, stationarity, wide Sense Stationarity (WSS),
                power spectral density, spectral factorization, linear time invariant
                processing of WSS random processes, ergodicity </li>
            </ol>
            <br>
            Ref: Therrien, Hayes, Papoulis, Ross<br>&nbsp
			</li>
          <li>Signal Modeling
            <ol>
              <li>LS methods, Pade, Prony (Deterministic methods)</li>
              <li>AR, MA, ARMA Processes (Stochastic approach), Yule-Walker Equations,
                Non-linear set of equations for MA system fit</li>
              <li> Harmonic Processes </li>
            </ol>
            <br>
            Ref: Hayes, Papoulis <br>&nbsp
          <li>Estimation Theory Topics
            <ol>
              <li>Random parameter estimation
                <ol>
                  <li>Cost function, loss function, square error, absolute error</li>
                  <li> Conditional mean (regression line) as the minimum mean
                    square error (MSE) estimator, orthogonality properties</li>
                  <li> Linear minimum mean square error (LMMSE) estimators, orthogonality
                    principle </li>
                  <li>Regression line, orthogonality </li>
                  <li>FIR, IIR, Causal–IIR Wiener filters</li>
                  <li>Linear Prediction, backward prediction</li>
                  <li>Random vector LMMSE estimation (multiple parameter)</li>
                </ol>
              </li>
              <li>Non-random parameter estimation
                <ol>
                  <li>Maximum likelihood method</li>
                  <li> Best Linear Unbiased Estimator (BLUE)</li>
                  <li>Discussion of linear estimators for the linear observation
                    model y=Ax+n</li>
                </ol>
              </li>
              <li>Karhunen – Loeve Transform</li>
            </ol>
            <br>
            Ref: Therrien, Hayes<br>&nbsp
        </ol>
        </li>
        <strong>References: </strong><br>
        <p class="style1">[Hayes]: M. H. Hayes, Statistical Signal Processing and Modeling, Wiley,
        New York, NY, 1996.</p>
        <p class="style1">[Therrien]: C. W. Therrien, Discrete random signals
          and statistical signal processing, Prentice Hall, c1992.</p>
        <p class="style1">[Papoulis]: A. Papoulis, Probability, Random Variables,
          and Stochastic Processes, 3rd edition, McGraw Hill, 1991. </p>
        <p class="style1">[Ross]: S. M. Ross, Introduction to probability models,
          7th ed. Harcourt Academic Press, 2000.</p>
        </td>
  </tr>
</table>

</div>
</body>

</html>
